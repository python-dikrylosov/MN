{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlA97xx0WOBu",
        "outputId": "199b1175-31a5-4132-bc7c-7db174cef1b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K83BqGGbmzG-"
      },
      "source": [
        "Подключаю модуль для работы с файла в диске"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj0aWRGLWS_6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import google.colab as colab\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF90hF7Jm6-b"
      },
      "source": [
        "подключаю нужные библиотеки для дальнейшей работы с данными"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRUMk-RHWTCa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "import joblib\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVlZyYwLnBpP"
      },
      "source": [
        "Непосредственно загрузка и обработка файла тесктовых данных для прочтения и дальнейшей обработки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnT4SVJ0WTEw"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "# Список файлов для загрузки\n",
        "data_path = \"/content/gdrive/MyDrive/NW/\"\n",
        "files = [\"2016_.xlsx\", \"2017_.xlsx\", \"2018_.xlsx\", \"2019_.xlsx\", \"2020_new_base_.xlsx\", \"2020_old_base_.xlsx\", \"2021_.xlsx\"]\n",
        "\n",
        "# Создаем пустой DataFrame для объединения данных\n",
        "data_frames = []\n",
        "\n",
        "# Чтение данных из файлов и добавление в список\n",
        "for file in files:\n",
        "    df = pd.read_excel(data_path+file)\n",
        "    #print(file)\n",
        "    #print(df.columns)\n",
        "    data_frames.append(df)\n",
        "\n",
        "\n",
        "# Объединение данных в один DataFrame\n",
        "data_full = pd.concat(data_frames, ignore_index=True)\n",
        "data = data_full[['Дата проведения','Наименование вида расходов','Направление расходов','Отнесено',]]\n",
        "data = data.rename(columns={'Дата проведения': 'Datetime'})\n",
        "\n",
        "#print(data)\n",
        "# Загрузка данных из файла Excel\n",
        "\n",
        "# Предварительная обработка данных\n",
        "# Преобразование категориальных данных (например, наименование видов расходов) в числовой формат\n",
        "label_encoders = {}\n",
        "for column in ['Наименование вида расходов', 'Направление расходов']:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    data[column + '_encoded'] = label_encoders[column].fit_transform(data[column])\n",
        "\n",
        "# Разделение на признаки и целевую переменную\n",
        "X = data[['Наименование вида расходов_encoded', 'Направление расходов_encoded']]\n",
        "y = data['Отнесено']\n",
        "print(X)\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Обучение модели\n",
        "model = RandomForestRegressor(n_estimators=20000, random_state=34)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Предсказание\n",
        "predicted_values = model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Absolute Percentage Error (MAPE)\n",
        "mape = 100 * (abs(y_test - predicted_values) / y_test).mean()\n",
        "print('Mean Absolute Percentage Error (MAPE):', mape)\n",
        "\n",
        "# Оценка качества модели\n",
        "mse = mean_squared_error(y_test, predicted_values)\n",
        "print('Mean Squared Error:', mse)\n",
        "\n",
        "# Выбор признаков для предсказания (если необходимо)\n",
        "X_test = data[['Наименование вида расходов_encoded', 'Направление расходов_encoded']]\n",
        "\n",
        "# Предсказание с использованием обученной модели\n",
        "predicted_values = model.predict(X_test)\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/NW/test_data.csv')\n",
        "print(data.shape)\n",
        "# Добавление предсказанных значений в поле \"Отнесено\"\n",
        "data['Отнесено'][0:4512] = predicted_values[0:4512]\n",
        "\n",
        "# Вывод первых строк данных для проверки\n",
        "print(data.head())\n",
        "\n",
        "data.to_csv('/content/gdrive/MyDrive/NW/test_data_pred_RF200000.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpXBIiw1nL8z"
      },
      "source": [
        "График вложений"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08BnhB-jmZDI"
      },
      "outputs": [],
      "source": [
        "plt.plot(data['Отнесено'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuZSQoVcmZIm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK51juBUnOu1"
      },
      "source": [
        "показ полученного датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5oztIc8WTHH"
      },
      "outputs": [],
      "source": [
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZgAdOk4nV7F"
      },
      "source": [
        "Первичная обработка для моделирования через линейную регрессию"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHFuZB1a1f1f"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "print(data)\n",
        "print(data.shape)\n",
        "#(4512, 4)\n",
        "#Наименование вида расходов\n",
        "#Направление расходов\n",
        "#Дата платежа\n",
        "#Отнесено спрогнозировать\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpWV_ITeNRXJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dOo28N6I5eF"
      },
      "outputs": [],
      "source": [
        "importimport pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as p\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "predicted_values = model.predict(X_test)\n",
        "\n",
        "# Model Evaluation\n",
        "mse = mean_squared_error(y_test, predicted_values)\n",
        "print('Mean Squared Error:', mse)\n",
        "\n",
        "# Calculate Mean Absolute Percentage Error (MAPE)\n",
        "mape = 100 * (abs(y_test - predicted_values) / y_test).mean()\n",
        "print('Mean Absolute Percentage Error (MAPE):', mape)\n",
        "\n",
        "# Visualization (optional)\n",
        "plt.scatter(y_test, predicted_values)\n",
        "plt.xlabel('True Values')\n",
        "plt.ylabel('Predictions')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHMTDHIwLkhk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/NW/test_data.csv')\n",
        "print(data)\n",
        "print(data.shape)\n",
        "#(4512, 4)\n",
        "#Наименование вида расходов\n",
        "#Направление расходов\n",
        "#Дата платежа\n",
        "#Отнесено спрогнозировать\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-0ft1Rd-LPx"
      },
      "outputs": [],
      "source": [
        "# Разделение данных на тренировочный и тестовый наборы\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Создание и обучение модели\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "# Сохранение обученной модели\n",
        "joblib.dump(model, 'linear_regression_model.pkl')\n",
        "# Прогнозирование коммунальных расходов\n",
        "predictions = model.predict(X_test)\n",
        "# Вычислить среднюю абсолютную ошибку\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print('Mean Absolute Error:', mae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSlDGhT8nfgA"
      },
      "source": [
        "визуализация обучения модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAuigIuR-Uoz"
      },
      "outputs": [],
      "source": [
        "# Визуализация исходных данных и прогнозов\n",
        "plt.scatter(X_test['Наименование вида расходов_encoded'], y_test, color='blue', label='Actual')\n",
        "plt.scatter(X_test['Наименование вида расходов_encoded'], predictions, color='red', label='Predicted')\n",
        "plt.xlabel('Наименование вида расходов_encoded')\n",
        "plt.ylabel('Utility Costs')\n",
        "plt.legend()\n",
        "plt.title('Linear Regression')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pb96ZJynkbL"
      },
      "source": [
        "Визуализация загруженной обученной модели линейной регрессии"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0owc-fsWTJe"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Загрузка сохраненной модели\n",
        "loaded_model = joblib.load('linear_regression_model.pkl')\n",
        "# Прогнозирование с использованием загруженной модели\n",
        "loaded_predictions = loaded_model.predict(X_test)\n",
        "print(loaded_predictions)\n",
        "\n",
        "# Визуализация прогнозов загруженной модели\n",
        "plt.scatter(X_test['Наименование вида расходов_encoded'], y_test, color='blue', label='Actual')\n",
        "plt.scatter(X_test['Наименование вида расходов_encoded'], loaded_predictions, color='green', label='Loaded Predicted')\n",
        "plt.xlabel('Investments')\n",
        "plt.ylabel('Utility Costs')\n",
        "plt.legend()\n",
        "plt.title('Loaded Linear Regression')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya3pQiZ3N7Je"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your data from a CSV file\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/NW/test_data.csv')\n",
        "\n",
        "# Data Preprocessing\n",
        "label_encoders = {}\n",
        "for column in ['Наименование вида расходов', 'Направление расходов']:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    data[column + '_encoded'] = label_encoders[column].fit_transform(data[column])\n",
        "\n",
        "# Feature and Target Variable Splitting\n",
        "X = data[['Наименование вида расходов_encoded', 'Направление расходов_encoded']]\n",
        "y = data['Отнесено']\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model Training\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "predicted_values = model.predict(X_test)\n",
        "\n",
        "# Model Evaluation\n",
        "mse = mean_squared_error(y_test, predicted_values)\n",
        "print('Mean Squared Error:', mse)\n",
        "\n",
        "# Calculate Mean Absolute Percentage Error (MAPE)\n",
        "mape = 100 * (abs(y_test - predicted_values) / y_test).mean()\n",
        "print('Mean Absolute Percentage Error (MAPE):', mape)\n",
        "\n",
        "# Visualization (optional)\n",
        "plt.scatter(y_test, predicted_values)\n",
        "plt.xlabel('True Values')\n",
        "plt.ylabel('Predictions')\n",
        "plt.title('True Values vs Predictions')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUSYP32Rnt0w"
      },
      "source": [
        "Проверка на ошибки средней абсолютной на крос-валидации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcZEt3_W_LuJ"
      },
      "outputs": [],
      "source": [
        "# Вычислить коэффициент детерминации\n",
        "r2 = r2_score(y_test, predictions)\n",
        "print('R-squared:', r2)\n",
        "# Оценка модели с помощью кросс-валидации\n",
        "scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
        "# Вычисление средней абсолютной ошибки на кросс-валидации\n",
        "mae_cv = -scores.mean()\n",
        "print('Mean Absolute Error (CV):', mae_cv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz90o6ton2Sa"
      },
      "source": [
        "Предсказание тренировочноого обученной модели линейной регрессии"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnmrSTR8_L5j"
      },
      "outputs": [],
      "source": [
        "# Создание объекта стандартизации\n",
        "scaler = StandardScaler()\n",
        "# Масштабирование данных тренировочного набора\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "# Масштабирование данных тестового набора\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# Создание и обучение модели на масштабированных данных\n",
        "model_scaled = LinearRegression()\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "# Прогнозирование коммунальных расходов на масштабированных данных\n",
        "predictions_scaled = model_scaled.predict(X_test_scaled)\n",
        "print(predictions_scaled)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3-4I3_pn_z8"
      },
      "source": [
        "Визуализация обучения модели  линейрной регрессии"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMlz8Cbu_TQL"
      },
      "outputs": [],
      "source": [
        "# Визуализация исходных масштабированных данных и прогнозов\n",
        "plt.scatter(X_test_scaled[:, 0], y_test, color='blue', label='Actual')\n",
        "plt.scatter(X_test_scaled[:, 0], predictions_scaled, color='red', label='Predicted')\n",
        "plt.xlabel('Scaled Investments')\n",
        "plt.ylabel('Utility Costs')\n",
        "plt.legend()\n",
        "plt.title('Scaled Linear Regression')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwuEDbwXoKjB"
      },
      "source": [
        "Проверка на обучение случайных лесов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG8ZXG4c_cKq"
      },
      "outputs": [],
      "source": [
        "# Создание объекта случайного леса\n",
        "rf_model = RandomForestRegressor()\n",
        "# Обучение модели на тренировочных данных\n",
        "rf_model.fit(X_train, y_train)\n",
        "# Сохранение обученной модели\n",
        "joblib.dump(rf_model, 'random_forest_model.pkl')\n",
        "# Прогнозирование коммунальных расходов на тестовых данных\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "# Оценка качества модели с помощью средней абсолютной ошибки\n",
        "rf_mae = mean_absolute_error(y_test, rf_predictions)\n",
        "print('Random Forest MAE:', rf_mae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqLHZXCjoPQt"
      },
      "source": [
        "Визуализация обучения модели случайного леса"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tb3RGVTC_cOD"
      },
      "outputs": [],
      "source": [
        "# Визуализация исходных данных и прогнозов случайного леса\n",
        "plt.scatter(X_test['Наименование вида расходов_encoded'], y_test, color='blue', label='Actual')\n",
        "plt.scatter(X_test['Наименование вида расходов_encoded'], rf_predictions, color='green', label='Predicted')\n",
        "plt.xlabel('Наименование вида расходов_encoded')\n",
        "plt.ylabel('Utility Costs')\n",
        "plt.legend()\n",
        "plt.title('Random Forest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsCfBBBuoUSl"
      },
      "source": [
        "средняя абсолютная ошибка градиентного бустинга"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYw0K1at_cQt"
      },
      "outputs": [],
      "source": [
        "# Создание объекта градиентного бустинга\n",
        "gb_model = GradientBoostingRegressor()\n",
        "# Обучение модели на тренировочных данных\n",
        "gb_model.fit(X_train, y_train)\n",
        "# Сохранение обученной модели\n",
        "joblib.dump(gb_model, 'gradient_boosting_model.pkl')\n",
        "# Прогнозирование коммунальных расходов на тестовых данных\n",
        "gb_predictions = gb_model.predict(X_test)\n",
        "# Оценка качества модели с помощью средней абсолютной ошибки\n",
        "gb_mae = mean_absolute_error(y_test, gb_predictions)\n",
        "print('Gradient Boosting MAE:', gb_mae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKQpJjZgocPM"
      },
      "source": [
        "визуализация градиентного бустинга"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xQTyZAZ_TWD"
      },
      "outputs": [],
      "source": [
        "# Визуализация исходных данных и прогнозов градиентного бустинга\n",
        "plt.scatter(X_test['Наименование вида расходов_encoded'], y_test, color='blue', label='Actual')\n",
        "plt.scatter(X_test['Наименование вида расходов_encoded'], gb_predictions, color='purple', label='Predicted')\n",
        "plt.xlabel('Наименование вида расходов_encoded')\n",
        "plt.ylabel('Utility Costs')\n",
        "plt.legend()\n",
        "plt.title('Gradient Boosting')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i15pO13PAMGs"
      },
      "outputs": [],
      "source": [
        "data_predict = pd.read_excel(\"/content/gdrive/MyDrive/NW/2022-3_Test_Demo.xlsx\")\n",
        "print(data_predict)\n",
        "print(data_predict.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqUB9ZgiSiA0"
      },
      "outputs": [],
      "source": [
        "данные_по_курсу = '/content/gdrive/MyDrive/NW/курс.xlsx'\n",
        "\n",
        "pd_kurs = pd.read_excel(данные_по_курсу)\n",
        "#print(pd_kurs)\n",
        "plt.title('Курс доллара с 2016 по 2023')\n",
        "plt.plot(pd_kurs['cdx'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4SuuaUkSjoM"
      },
      "outputs": [],
      "source": [
        "!pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQsdZWMESjls"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import time\n",
        "real_date = time.strftime(\"%Y-%m-%d\")\n",
        "RUBX = yf.download(\"RUB=X\",start='2000-01-01',end=real_date,interval=\"1mo\")\n",
        "RUBX_close = RUBX['Close']\n",
        "plt.plot(RUBX_close)\n",
        "#plt.plot(data['Отнесено'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3gukR6LSjtO"
      },
      "outputs": [],
      "source": [
        "#EXPERIMNETS NEIRO LSTM DENSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXyY1AbPYZyu"
      },
      "outputs": [],
      "source": [
        "data_predict = pd.read_excel(\"/content/gdrive/MyDrive/NW/2022-3_Test_Demo.xlsx\")\n",
        "print(data_predict)\n",
        "print(data_predict.shape)\n",
        "time.sleep(10)\n",
        "for i in range(data_predict.shape[0]):\n",
        "    print(data.shape)\n",
        "    data2 = data.filter(['Summa'])\n",
        "    dataset = data2.values\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(dataset)\n",
        "    print(scaled_data)\n",
        "    # получение цифровых строк для обучения модели\n",
        "    training_data_len = math.ceil(len(dataset) * .8)\n",
        "    print(training_data_len)\n",
        "    # создание обучающего набора данных\n",
        "    train_data = scaled_data[0:training_data_len, :]\n",
        "\n",
        "    dataset = data2.values\n",
        "    print(dataset)\n",
        "    # print(dataset)\n",
        "    import math\n",
        "    # получение цифровых строк для обучения модели\n",
        "    training_data_len = math.ceil(len(dataset) * .8)\n",
        "    print(training_data_len)\n",
        "\n",
        "    # масштабирование данных\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(dataset)\n",
        "    print(scaled_data)\n",
        "\n",
        "    # создание обучающего набора данных\n",
        "    train_data = scaled_data[0:training_data_len, :]\n",
        "\n",
        "\n",
        "    # разделение данных на наборы данных x_train и y_train\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    for i in range(60, len(train_data)):\n",
        "        x_train.append(train_data[i - 60:i, 0])\n",
        "        y_train.append(train_data[i, 0])\n",
        "        if i <= 61:\n",
        "            print(x_train)\n",
        "            print(y_train)\n",
        "            print()\n",
        "\n",
        "    # преобразование x_train и y_train в массивы numpy\n",
        "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "    # bot.send_message(CHANNEL_NAME, (x_train, y_train))\n",
        "\n",
        "    # reshape the data\n",
        "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
        "    print(x_train.shape)\n",
        "\n",
        "    # biuld to LSTM model\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(LSTM(5, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
        "    model.add(LSTM(1))\n",
        "    model.add(Dense(data2.shape[0]))\n",
        "\n",
        "    # compale the model\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    # train_the_model\n",
        "    from tensorflow import keras\n",
        "\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=60, epochs=1)\n",
        "    # create the testing data set\n",
        "    # create a new array containing scaled values from index 1713 to 2216\n",
        "    test_data = scaled_data[training_data_len - 60:, :]\n",
        "\n",
        "    # create the fata sets x_test and y_test\n",
        "    x_test = []\n",
        "    y_test = dataset[training_data_len:, :]\n",
        "    for i in range(60, len(test_data)):\n",
        "        x_test.append(test_data[i - 60:i, 0])\n",
        "\n",
        "    # conert the data to numpy array\n",
        "    x_test = np.array(x_test)\n",
        "    # reshape the data\n",
        "    X_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
        "\n",
        "    # get the model predicted price values\n",
        "    predictions = model.predict(X_test)\n",
        "    predictions = scaler.inverse_transform(predictions)\n",
        "\n",
        "    # get the root squared error (RMSE)\n",
        "    rmse = np.sqrt(np.mean(predictions - y_test) ** 2)\n",
        "    print(rmse)\n",
        "\n",
        "    # get the quate\n",
        "\n",
        "    new_df = data2.filter(['Summa'])\n",
        "\n",
        "    # get teh last 60 days closing price values and convert the dataframe to an array\n",
        "    last = new_df[-1:].values\n",
        "    # scale the data to be values beatwet 0 and 1\n",
        "\n",
        "    last_scaled = scaler.transform(last)\n",
        "\n",
        "    # creAte an enemy list\n",
        "    X_test = []\n",
        "    # Append past 60 days\n",
        "    X_test.append(last_scaled)\n",
        "\n",
        "    # convert the x tesst dataset to numpy\n",
        "    X_test = np.array(X_test)\n",
        "    # Reshape the dataframe\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "    # get predict scaled\n",
        "\n",
        "    pred_price = model.predict(X_test)\n",
        "    # undo the scaling\n",
        "    pred_price = scaler.inverse_transform(pred_price)\n",
        "    print(pred_price)\n",
        "    pred_price = np.array(pred_price)\n",
        "    pred_price = pred_price[0]\n",
        "    pred_price = pred_price[0]\n",
        "    # Отсылаем юзеру сообщение в его чат\n",
        "    print(pred_price)\n",
        "    data_predict[\"Отнесено\"][i] = pred_price\n",
        "    data_predict.to_csv(\"/content/gdrive/MyDrive/NW/predict_v1.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ID9OdUEoSiD9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "data = pd.read_csv('test_data.csv')\n",
        "print(data)\n",
        "print(data.shape)\n",
        "\n",
        "for i in range(data.shape[0]):\n",
        "    data['Отнесено'][i] = random.randint(0,100000)\n",
        "\n",
        "print(data)\n",
        "data.to_csv('test13.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FD2pyyftQcbl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViR2_yuNQdcw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyVf2ju0ifWa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}